## Learning Goals
* Be able to explain the difference between Regression and Classification?
* Explain each of the following
    * logistic regression
        * multinomial logistic regression
    * linear discriminant analysis
    * quadratic discriminant analysis
    * naive Bayes
    * K-nearest neighbors
    * generalized linear models,
    * Poisson Regression
* Explain how logistic regression can be used as a jump-off point to
    * generalized linear models,
    * Poisson Regression
* Explain the difference between quantitative and qualitative (categorical) variables
* Explain what Classification is
    * It's the predicting of qualitative variables for an observation
* Explain how Classification is related to Regression
    * Often we are interested in assigning probabilities of a given observation being part of a category and then make the decision based on that, in that sense we're doing regression
* Explain the Bias-variance trade-off.

## Vocabulary
* `logistic` regression and function
* multinomial logistic regression
* `odds`
* `logit` / `log odds`
* Categorical / Qualitative Variables
* Quantitative Variables
* Maximum likelihood
* Likelihood Function
* z-statistic
* `softmax`
* Bayes' Theorem
* Bayes' Classifier
* Bayes' Decision Boundary
* Linear Discriminant Analysis
* Overfitting
* (Trivial) Null Classifier
* Confusion Matrix
* Sensitivity
* Specificity
* ROC Curve (receiver operating characteristics)
* Naive Bayes classifier
* Marginal Distribution
* Joint Distribution
* Bias-variance trade-off.
* Generalized additive model in the context of Naive Bayes classifier
* Cross Validation

## Theorems and Equations
### Likelihood Function
$$
L(\beta_0, \beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
$$

## Notes
* The Reason why we use classification instead of regression with a numerical encoding of the predictors is, among other things, that it induces a type of ordering, even if it does not make sense for our data. In particular a different labeling would imply an entirely different type of relationship between the predictors and the response variable.
* The only case where this works well is in the case of binary classification, i.e. where we have two different categories. It is easy to see that if we flip the categories we get the same underlying relationship between the predictors and the response variable.
* Logistic Regression models the probability that a particulary piece of data is a member of a particular category.
* The obvious problem with just doing a linear model is that there is no assurance that (and in fact it's not true in general) that our probabilities lie in $[0, 1]$.
    * We can solve this problem by using the logistic function (which is where the name comes from)
* QDA and Naive Bayes aren't special cases of each other because while naive bayes allows arbitrary functions $g_{kj}(x_j)$ it only allows for addition of those, but to cover QDA we'd need to introduce multiplication (because we have $x_ix_j$ terms).